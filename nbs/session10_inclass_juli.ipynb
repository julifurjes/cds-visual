{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 10 - Image search with VGG16 and K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base tools\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(\"..\"))\n",
    "\n",
    "# data analysis\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from tqdm import notebook\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import (load_img, \n",
    "                                                  img_to_array)\n",
    "from tensorflow.keras.applications.vgg16 import (VGG16, \n",
    "                                                 preprocess_input)\n",
    "# from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What kind of preprocessing am I doing here? Why do you think I'm doing it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img_path, model):\n",
    "    \"\"\"\n",
    "    Extract features from image data using pretrained model (e.g. VGG16)\n",
    "    \"\"\"\n",
    "    # Define input image shape - remember we need to reshape\n",
    "    input_shape = (224, 224, 3)\n",
    "    # load image from file path\n",
    "    img = load_img(img_path, target_size=(input_shape[0], \n",
    "                                          input_shape[1]))\n",
    "    # convert to array\n",
    "    img_array = img_to_array(img)\n",
    "    # expand to fit dimensions\n",
    "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "    # preprocess image - see last week's notebook\n",
    "    preprocessed_img = preprocess_input(expanded_img_array)\n",
    "    # use the predict function to create feature representation\n",
    "    features = model.predict(preprocessed_img)\n",
    "    # flatten\n",
    "    flattened_features = features.flatten()\n",
    "    # normalise features\n",
    "    normalized_features = flattened_features / norm(features)\n",
    "    return normalized_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', \n",
    "              include_top=False,\n",
    "              pooling='avg',\n",
    "              input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 210ms/step\n"
     ]
    }
   ],
   "source": [
    "features = extract_features('../data/img/florence.jpg', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../flowers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# path to the datasets\u001b[39;00m\n\u001b[1;32m      2\u001b[0m root_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mflowers\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m filenames \u001b[39m=\u001b[39m [root_dir \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(root_dir))]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../flowers'"
     ]
    }
   ],
   "source": [
    "# path to the datasets\n",
    "root_dir = os.path.join(\"..\", \"flowers\")\n",
    "filenames = [root_dir + \"/\" + name for name in sorted(os.listdir(root_dir))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract features for each image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m feature_list \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m notebook\u001b[39m.\u001b[39;49mtqdm(\u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(filenames))):\n\u001b[1;32m      3\u001b[0m     feature_list\u001b[39m.\u001b[39mappend(extract_features(filenames[i], model))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tqdm/notebook.py:238\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m unit_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    237\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39m*\u001b[39m unit_scale \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n\u001b[0;32m--> 238\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatus_printer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp, total, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdesc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncols)\n\u001b[1;32m    239\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mpbar \u001b[39m=\u001b[39m proxy(\u001b[39mself\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tqdm/notebook.py:113\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[39m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m IProgress \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m total:\n\u001b[1;32m    115\u001b[0m     pbar \u001b[39m=\u001b[39m IProgress(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "feature_list = []\n",
    "for i in notebook.tqdm(range(len(filenames))):\n",
    "    feature_list.append(extract_features(filenames[i], model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our *database* of extracted embeddings, we can then use K-Nearest Neighbours to find similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "neighbors = NearestNeighbors(n_neighbors=10, \n",
    "                             algorithm='brute',\n",
    "                             metric='cosine').fit(feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate nearest neighbours for target__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = neighbors.kneighbors([feature_list[250]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Save indices, print data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = []\n",
    "for i in range(1,6):\n",
    "    print(distances[0][i], indices[0][i])\n",
    "    idxs.append(indices[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot target image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mpimg.imread(filenames[250]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot close images__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mpimg.imread(filenames[251]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot target and top 3 closest together__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt target\n",
    "plt.imshow(mpimg.imread(filenames[251]))\n",
    "\n",
    "# plot 3 most similar\n",
    "f, axarr = plt.subplots(1,3)\n",
    "axarr[0].imshow(mpimg.imread(filenames[idxs[0]]))\n",
    "axarr[1].imshow(mpimg.imread(filenames[idxs[1]]))\n",
    "axarr[2].imshow(mpimg.imread(filenames[idxs[2]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple style transfer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load a quick style transfer model from TF Hub__\n",
    "\n",
    "You can find more details [here](https://www.tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load TF-Hub module.\n",
    "hub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'\n",
    "hub_module = hub.load(hub_handle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load the content image and the style image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = st_load(\"../data/img/florence.jpg\")\n",
    "style_image = st_load(\"../data/img/starry_night.jpg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Process using the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = hub_module(content_image, style_image)\n",
    "stylized_image = outputs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Show content, style, and stylized image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_n([content_image, style_image, stylized_image], \n",
    "       titles=['Original content image', 'Style image', 'Stylized image'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1 \n",
    "- Run this same pipeline on the Indo Fashion dataset. How does it perform?\n",
    "\n",
    "### Task 2\n",
    "- Take the code in this notebook and turn it into a Python script. You can then add this to the repo for your Assignment 1 solution for creating doing image search\n",
    "  - I.e. your Assignment 1 repo would contain both code for image search using colour histograms *and* for image search using a pretrained CNN.\n",
    "\n",
    "### Task 3 \n",
    "- Continue working on Assignment 3 in-class just now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
